---
title: "Week 6 Materials"
author: "Matt Richards"
date: "8/9/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Week 6 Overview

There wasn't a lot of new functionality introduced this week; really, the focus was more on how to take the joins you've already learned and add options to them. There were a couple new functions that I'll summarize, but if anything they're more supplementary:

* `reduce()`: a function from the `purrr` package that allows you to perform successive operations on a list or vector. In the slides, they demonstrated using `reduce()` on the "join" functions, but you can do it on something stupidly simple too:

```{r message=FALSE}
library(purrr); library(dplyr);
reduce(1:10, `*`)
```

Notice that I had to use back ticks here to surround the multiplication operator. You can pipe stuff to `reduce()` as well; at this point, I think you can do pretty much everything with pipes.

* `merge()`: I may have mentioned this in class before; the equivalent of the `join()` family of functions is the `merge()` function. It can do all of the same stuff, but it's not necessarily as specific and/or intuitive. The term "left join" actually means a specific thing, as you've seen, but "merge" is more arbitrary. So if you decide to use `merge()` (and I definitely have), then make sure you're careful about your options

## Additional Materials: Using Databases

In the DataCamp lectures, they briefly talked about how you can directly use `dplyr` to do things with databases. As with all `dplyr` functionality, this is essentially duplicated by other functions, in this case those available in the `DBI` package. The big difference is that with `dplyr`, you don't really need to know any SQL. However, basic SQL isn't that hard, and it's a valuable skill for working with databases. Let's go over that here.

For these exercises (and the ones that follow) you'll need the `DBI` and `RSQLite` packages. Once those are installed, you should have everything you need. For the examples here, we'll use the `Lahman` package data, which I've installed on my local machine. You can do that too if you'd like; it should look familiar if you've been doing the DataCamp exercises. 

The `Lahman` data contain all sorts of information about baseball players and the main table is called `Master`. There's also a table called `Batting` that's relevant to what we're doing here. This dataset is nice because it's all formatted and perfect, but most data isn't already nice and formatted in an R package. Thus, rather than load it from the dataset itself, we're going to use a mini version in an SQLite database that I created for you in this folder. 

### Connecting to and Surveying a Database

The first thing you need to do is actually find your data. In this case, it's a SQLite database, which is the easiest type to work with. SQLite databases are just files; if you look in this folder, you'll see them:

```{r}
dir()
```

As you can see, I made a little database called `toy.lahman.db` for us to work with. We'll get into how I did that later. For now, we want to connect to it and the nice thing about a simple SQLite database is I don't need any special credentials, just the fact that it's SQLite and the name of the database. To do this, we'll use the `dbConnect()` function from `DBI`:

```{r message = FALSE}
library(RSQLite)
con <- dbConnect(SQLite(), dbname = "toy.lahman.db")
```

We've now created a connection-type object called `con` that we'll need to use for all of our database activities. Of note here: if we were connecting to a different sort of database, we'd probably need a user name and password, and if the database were located elsewhere, we'd have to specify a host name as well. So if we were hitting a PostgreSQL database called `My.Db` with username `Matt` and password `MyPass` on a host called `My.Host`, it would look like this:

```{r eval = FALSE}
library(RPostgreSQL)
con2 <- dbConnect(PostgreSQL(), 
                  user = "Matt", 
                  password = "MyPass",
                  host = "My.Host",
                  dbname = "My.Db")
```

Don't actually evaluate this code as it will go nowhere, but that's how the syntax would work. ALWAYS specify what each additional argument is; only the database driver (in this case `PostgreSQL()`) has a specified place (first) in the function. 

We can now examine what's in our database using the `con` object. For instance, here's how we'd figure out what tables are there:

```{r}
dbListTables(con)
```

Look familiar? It should! We can also see what fields are in a given table:

```{r}
dbListFields(con, name = "Batting")
```

What we see here is that we've got a bunch of different batting statistics for a given playerID and a given yearID. So this is all well and good, but now we need to actually pull out some data. How the heck do we do that? It's time to learn about queries!

### Writing a "Query"

If you've never seen a query before, they might look quite weird, but they're actually pretty straightforward. The basic syntax is something like:

```select [columns] from [table] where [conditions] [additional options];```

There's PLENTY of documentation on this available, but I'll do my best to give examples of everything you might need. The first thing is how you grab an entire table; the "*" operator means to take all the columns from a table, so I'd do something like this to get the whole `Batting` table:

```select * from Batting;```

Of course, then I'll be overloaded with about 100,000 rows, so I want a way to limit the number of rows; here's how I do that to, say, get only the first 10 rows:

```select * from Batting limit 10```

I can also grab only certain columns by trading the `*` for a list of column names that are separated by commas:

```select AB, BB, GIDP from Batting;```

All of these have been grabbing the whole dataset, but I can also filter things out based on conditions using the `where` argument. For instance, I can grab only players with at least 50 home runs (HR) in a season:

```select * from Batting where HR >= 50;```

I can also do something like this using character strings, but I need to surround them in single quotes:

```select * from Batting where playerID == 'addybo01';```

Finally, you can also search for substrings using the "like" operator, which also requires use of percentage signs (`%`) as follows:

```select * from Batting where playerID like '%aaron%';```

That grabs all entries where the playerID contains the substring "aaron". And that should be all you neeed in terms of queries; we won't cover joins in SQL because you already know them in `dplyr`, so stick to that. 

You might have noticed to this point that I haven't actually been using R; these queries are literally in the SQL language, and to actually use them we need to use the function `dbGetQuery()` in R. So let's grab the first 5 rows of the `playerID` and `yearID` variables:

```{r}
my.query <- "select playerID, yearID from Batting limit 5;"
dbGetQuery(con,my.query)
```

As you can see, this returns a data frame to me with the data I wanted. Let's go a step further and grab all player seasons where the player hit at least 50 home runs:

```{r}
my.query <- "select playerID, yearID, HR from Batting where HR >= 50"
hr.50 <- dbGetQuery(con, my.query)
head(hr.50)
```

One more word on this; the first command I showed was able to grab an entire table, but the `DBI` package actually has a more straightforward way to grab an entire table using `dbReadTable()`:

```{r}
all.batting <- dbReadTable(con, name = "Batting")
```

You'll notice here that I now have a data frame that I can now play around with and manipulate in R. This is the goal of the whole operation: grab the data you want and pull it into R so you can find useful stuff with it. At this point, it's up to you what you want to do with the data.

### Writing to a Database

Alright, so you've gotten your data and presumably done something with it. Maybe now you feel like writing it to a new database, where other people can access it. The useful thing about databases as opposed to R objects is that pretty much anyone can access a database if they have the right credentials, they don't actually need to know R to do it. And unlike a text file, things in a database are actually linked together, not merely lined up next to one another.

To write a table to a database, you do something like this (this is how I wrote the "Batting" table to the mini database)

```{r eval = FALSE}
dbWriteTable(con, "Batting",batting.df, overwrite = TRUE)
```

Basically, I tell it to use my `batting.df` data frame and write it to a table called "Batting", plus to overwrite any existing table of that name in the database. Alternatively, you could just append to an existing table, but that can have its own issues. 

At this point, you're basically done. The only thing left to do: disconnect from the database as follows:

```{r}
dbDisconnect(con)
```

That's it, now your connection is closed. 

### Summary of SQL Stuff

To sum up, here's the basic workflow of how you can grab data from a database, do stuff to it, and then write it to a new database:

1. Connect to your database(s) of interest using `dbConnect()` and the correct credentials
2. Identify the tables you're interested in using `dbListTables()` and the columns you want with `dbListFields()`
3. Craft a "query" that grabs the correct information using the `select [column1,column2]|* from [table] {where [condition]};` syntax
4. Run the query using `dbGetQuery()` to store the information in a variable
5. Assuming you're done with your database, close the connection with `dbDisconnect()`
6. Do whatever data manipulation you'd like using `dplyr` or other methods
7. Once you've got a data frame to write to a database, open up a connection to write to using `dbConnect()` and any necessary credentials
8. Write your table using `dbWriteTable()`, which turns the dataframe into a database table
9. Close your connection with `dbDisconnect()`

## In Class Exercises

We're going to attempt to come full circle on the motif exercises we've been doing to this point. The whole reason I've been working with motifs is because we're interested in figuring out where motifs pop up in the human genome. Essentially, we took 2 databases:

1. A mapping of every gene motif to every place it appears in the human genome
2. A mapping of places where transcription factors (TFs) are likely to bind in a given tissue

So if I take, say, brain tissue, I've got a map of everywhere I think a TF binds in that tissue. I can essentially join that to the first mapping, which has all the places in the human genome where ANY TF binds for ANY tissue. The result is:

* A database of every site where we think a TF binds for a given tissue, including the location, the motif, and the score of the probability of binding. 

Make sense? I've taken a map of all the places where TFs and crossed it with a map of locations for a given tissue. It's as if I had:

1. A list of the names and street addresses for every Green Party voter in the US
2. A list of street addresses that correspond only to a particular state

If I cross those, I can get a state-specific mapping of every Green Party voter, including their name and address. If I only have the 1st, then my list isn't state specific; if I only have the 2nd, I don't know the people's names. So by crossing the 2, I get a new, specific piece of information. 

Likewise, I knew the names and locations of all binding sites in the human genome and I knew the list of binding sites that correspond to a given tissue. So now I know exactly which names and locations correspond to each particular tissue. If I do this for lots of tissues, I can start to find differences in where TFs bind or in which TFs are binding there. 

A final piece of important information: that second mapping (where TFs bind in a given tissue) is determined by one of 2 methods. One ("Wellington") is more conservative, so we get fewer locations, and the other ("Hint") is more liberal. So based on which method we choose, we'll get more or fewer locations in our particular tissue. 

I have effectively taken small snapshots of some databases I created for the "bone element" tissue. These snapshots are of just the Y chromosome because it's the smallest chunk I could break off. One database corresponds to using the "Wellington" method and the other corresponds to the "Hint" method; you can find both under the [Datasets](https://github.com/marichards/FH_intermediate_R/tree/master/Datasets) folder as `well.bone.db` and `hint.bone.db`, respectively. 

### Here's what I'd like you to attempt today:

1. Take a look at one of the databases (they're structured the same) and answer the following questions:
    * What tables do you have? 
    * What fields are in each table? 
    * How are the tables in the database related to one another? 
    * Why are they structured like they are?

2. How many unique motifs appear in the respective databases? How do they compare (Hint: the motifs generally look like `MA0001.1`, `MA0002.1`, etc)?

3. How many total motifs (not just unique ones) appear in the respective databases? How do those numbers compare?

4. How many motifs (just the names) appear in both datasets? Which ones don't appear in HINT but do appear in Wellington?

5. How many rows are identical between the each of the 2 sets of matching tables? (i.e. what did the methods both find that are exactly the same?)

6. Let's narrow it down even further; how many hits are found between the 25,000,000 and 30,000,000 positions? That is, the region needs to start AFTER 25 million bp and end BEFORE 30,000 positions. This is NOT quite the same as the `fp_start` and `fp_end` columns; we're specifically talking about the `start` and `endpos` columns of the `regions` table. How many hits do you find for each method? 

7. Using our 2 new data sets from step 6, put the 2 together into one data frame. Then, use your `dplyr` skills to pull out these statistics for each method:
    * Mean, median, min, and max of the "score1" column
    * Mean, median, min, and max of the "length" column
    * Number of unique motifs for each method
    
What can you say about how the methods compare? Anything interesting?

8. Using the data from 7 to guide your intution about what's interesting, make a plot that illustrates a compelling comparison between the methods. It's open-ended!

### BONUS EXERCISE

A lot of these commands can be done directly in SQL; try doing step 6 using the R-SQL connection directly rather than `dplyr`. If done correctly, you should get the same exact answer. 

```{r echo = FALSE, eval = FALSE}
# Here's how to read them into dataframes:
well.con <- dbConnect(SQLite(), dbname = "../Datasets/well.bone.db")
hint.con <- dbConnect(SQLite(), dbname = "../Datasets/hint.bone.db")
well.hits <- dbGetQuery(well.con, "select * from hits;")
hint.hits <- dbGetQuery(hint.con, "select * from hits;")
well.regions <- dbGetQuery(well.con, "select * from regions;")
hint.regions <- dbGetQuery(hint.con, "select * from regions;")

# Here's how to check the tables and fields (question 1)
dbListTables(well.con) # hits, regions
dbListTables(hint.con) # hits, regions
dbListFields(well.con, name = "hits") # Lots of things
dbListFields(well.con, name = "regions") # Fewer things; relates by "loc" column

# Here's how to check the rows in each (last 2 are for question 3)
nrow(hint.regions)
nrow(well.regions)
nrow(hint.hits) #32649
nrow(well.hits) #13240

# Here's how to check unique motifs in the hits (question 2)
length(unique(well.hits$name)) #1333
length(unique(hint.hits$name)) #1482

# Here's how to find those in common and the ones only in wellington (question 4)
length(intersect(well.hits$name, hint.hits$name)) # 1328
setdiff(well.hits$name, hint.hits$name) # 5 names I don't want to deal with here

# Here's how to find all rows in common (question 5)
length(intersect(well.hits, hint.hits)) # 0
length(intersect(well.regions, hint.regions)) # 0

# Here's how to join them together and grab the specified positions (question 6)
spec.well <- well.hits %>% left_join(well.regions, by = "loc") %>% filter(start > 25000000, endpos < 30000000) 
spec.hint <- hint.hits %>% left_join(hint.regions, by = "loc") %>% filter(start > 25000000, endpos < 30000000) 
nrow(spec.well) # 679
nrow(spec.hint) # 2345

# Here's how to put together the datasets and pull out grouped summary statistics:
my.stats <- bind_rows(spec.well, spec.hint) %>% 
  select(name, length, method, score1) %>%
  group_by(method) %>%
  summarise(Unique.motifs = n_distinct(name),
            Mean.score = mean(score1),
            Med.score = median(score1),
            Max.score = max(score1),
            Min.score = min(score1),
            Mean.length = mean(length),
            Med.length = median(length),
            Max.length = max(length),
            Min.length = min(length)
            )

# Here's an example plot looking at the distribution of scores v. length
bind_rows(spec.well, spec.hint) %>%
  ggplot(aes(x = length, y = score1)) + geom_point(aes(col = method))
  
```


