---
title: "Intro to Parallel Programming in R"
author: "Matt Richards"
date: "5/17/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

One of the many benefits of modern computing is that you generally have multiple cores at your disposal, even if you're just on your own laptop. These multiple cores enable you to run tasks in parallel, cutting down on required compute time. Though this gain in speed might not seem like much if you're just running simple operations, it can be hugely helpful when working on more complex, lengthy tasks. In this document, we'll briefly look at some tools for parallel processing in R and demonstrate how they work.

## The doParallel Package

There are a number of different packages here; we'll be using the `doParallel` package as our main engine. This package contains tools for both setting up and executing our parallel tasks, and it's the package I use for parallelization in my own work. Before going any further, you should install it on your machine in the usual manner. Then we'll load it up:

```{r}
library(doParallel)
```

In this document, we will touch upon some of what you can do with the `doParallel` package, but for a full set of documentation you can checkout the `doParallel` [vignette](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf) or [reference manual](https://cran.r-project.org/web/packages/doParallel/doParallel.pdf).

## The basic workflow

I would summarize running a parallel process as essentially a 2-step process: 

1. Setup and register a cluster
2. Run your operations in loop-like notation

I'll go through each of these steps separately, then we'll put them together for a few quick examples

### Registering a cluster

Your machine has multiple cores, but it won't know to use them in parallel unless you tell it to. We'll do this using the `makeCluster` command to tell our machine to create a computational cluster that runs cores in parallel. But wait! How many cores should we use? It helps to know how many cores we have at our disposal, a figure we can obtain using `detectCores()`:

```{r}
detectCores()
```

As you can see, I'm using a machine with 4 cores, but if you're on a dedicated cluster or something of that nature, you may very well find yourself with many more. In any case, the follow-up question is: how many of my cores should I actually use? In general, if I don't have any other constraints, I'll use my maximum number of cores minus 1. So to setup my parallel process, I would run:

```{r}
cl <- makeCluster(detectCores() - 1)
cl
```

Now I have a cluster called `"cl"` and as you can see, it's a cluster with 3 nodes. My cluster now exists, but I still need to tell my machine to use it by registering the cluster as follows: 
```{r}
registerDoParallel(cl)
```

And that's it for the setup; my cluster is primed and ready to go, with 3 workers to do parallel tasks. 

### Run your operations like a loop

Though there are multiple ways of running parallel processes, we'll be using the `foreach` package, which was already loaded with `doParallel`. The `foreach` function looks a lot like a `for` loop, with a few differences. For instance, if I wanted to use a `for` loop to do something simple like, say, take a short list of numbers and increment each number by 1, I could do something like this:

```{r}
my.list <- 1:10
for(i in 1:length(my.list)){
  my.list[i] <- my.list[i] + 1
}
my.list
```

The parallel way of doing this looks pretty much the same, but there are a few tweaks:

```{r}
my.list <- 1:10
my.list <- foreach(i = 1:length(my.list)) %dopar% {
  my.list[i] <- my.list[i] + 1
}
unlist(my.list)
stopCluster(cl)
```

Some things to note here:

* The `foreach` declaration uses the `i =` convention rather than `i in` convention
* The `%dopar%` statement is critical; you can alternatively say `%do%` instead, but then it won't run in parallel
* The `foreach` method requires that I assign an output value; if I don't, I won't capture my output
* Using `foreach` returns a list
* A `foreach` "loop" operates like a function, returning the last statement or a value captured using `return(value)`
* I used the `stopCluster` function at the end to terminate the cluster, preventing it from possibly gumming up the works

Overall, using parallel processing requires a bit more typing than regular processing, so you wouldn't ever use it for something as trivial as this. But what you invest in setup can payoff quite a bit if you're doing some big, time-intensive task. 

### Another Example

Oftentimes, you'll find yourself with a function that takes awhile to run. If you need to run said function for a whole bunch of cases, parallelization could be the answer. For instance, let's suppose I have the following function:

```{r}
delayFunction <- function(x){
  Sys.sleep(5)
  return(abs(round(100*x)))
}
```

This is admittedly a pretty dumb function; it tells the machine to delay for 5 seconds, then returns the rounded absolute value of a number, x. But it should at least illustrate the reason we parallelize. Let's first run this with a serial loop; we'll time execution of the function using the `system.time()` function, a useful little utility that I use from time to time. 

```{r cache = TRUE}
set.seed(10)
x <- rnorm(10)
y <- numeric(length=10)
system.time(for(i in 1:length(x)) {
  y[i] <- delayFunction(x[i])
}
)
```

Now let's do exactly the same thing, but use a parallel structure with 3 cores:

```{r cache = TRUE}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
system.time(
  z <- foreach(i = 1:length(x)) %dopar% {
    delayFunction(x[i])
}
)
stopCluster(cl)
```

First things first: that's a pretty noticable improvement in speed, right? This was just a case of a silly delay function working on 10 variables, and we more than doubled our efficiency. Suppose you had a more complex function that took, say, a couple minutes per variable and needed to be run on a few thousand variables; suddenly, the 30 second speed-up we have becomes hours or even days shaved off your compute time. 

You may have noticed that I assigned different variables for the output of the function; that's so we could verify that we get exactly the same thing, regardless of method:

```{r}
all(y == unlist(z))
```

I would also note that this method compares favorably to using `lapply` as well:

```{r cache = TRUE}
system.time(w <- unlist(lapply(x,delayFunction)))
all(w == y)
```

So although the `lapply` method saves typing, it's clearly not saving time here. However, if you really want to, there are parallelized versions of the `apply` functions as well. I won't cover them here because I think `foreach` is generally sufficient for your purposes, but if you're curious, check out the documentation for `parLapply` and much more [here](https://stat.ethz.ch/R-manual/R-devel/library/parallel/html/clusterApply.html).

Cool, so to sum up, we shaved some time off using our function and still got the same answer. Even though we used an admittedly dumb function, you can imagine that this would come in pretty handy if you had a longer, more complicated function. 